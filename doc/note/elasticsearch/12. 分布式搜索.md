执行分两个阶段：查询然后取回
# 查询阶段

![](http://git.oschina.net/wzj777/princeWiki/raw/master/pic/elasticsearch/es-2.png)

1. 发送请求给node3 {from:90, size:10}

2. node3广播请求给其他的分片，每个分片本地计算优先队列都给node3. 优先队列就是匹配的最少的结果集，这个例子就是100个。

3. node3进行汇总，然后取第90-100个产生全局排序的结果。





# 取回阶段

![](http://git.oschina.net/wzj777/princeWiki/raw/master/pic/elasticsearch/es-3.png)

1. 协调节点全排序之后知道哪些需要取回，然后向相关分片发送取回请求

2. 其他节点把取回结果发给协调节点

3. 所有文档都取回，则返回给客户端

# 深分页

就是需要过滤掉的新闻太多了。所以不行，目前最大是10000



# 搜索选项

curl -XGET 'localhost:9200/thecover/_search?preference=_primary&pretty'

preference偏爱

timeout 超时

routing 限制只搜索哪些分片而不是全部分片

        使用post添加索引的时候就需要用这个参数，然后查询的时候使用相同的，就能跟原来添加的时候使用同一个分片了。  另外因为默认是通过id来算分片的，所以这个值默认可以用id来做。具体的没有搞太懂。


curl -XGET 'localhost:9200/thecover/_search?search_type=count&pretty' -d'{

    "query":{

        "match":{"title":"first blog"}

    }

}'

search_type=count 统计总数  默认值是query_then_fetch

# 扫描和滚屏

高效取出大量结果，不用付出深分页的代价


curl -XGET 'localhost:9200/thecover/_search?search_type=scan&scroll=1m&pretty' -d'{

    "query":{

        "match":{"title":"first blog"}

    }

}'

得到一个_scroll_id然后



curl -XGET 'localhost:9200/_search/scroll?scroll=1m&pretty' -d'

c2Nhbjs1OzMxMzpGRFBjdzBKZVEyQ1JlTldZNmtaeGFnOzMxNDpGRFBjdzBKZVEyQ1JlTldZNmtaeGFnOzMxNTpGRFBjdzBKZVEyQ1JlTldZNmtaeGFnOzMxNjpGRFBjdzBKZVEyQ1JlTldZNmtaeGFnOzMxNzpGRFBjdzBKZVEyQ1JlTldZNmtaeGFnOzE7dG90YWxfaGl0czo3Ow=='


意思是从此刻开始准备滚屏， 然后给了一个scrollid，用这个id来查看刚才那个时间店内增加的文档。

具体等用到了再说