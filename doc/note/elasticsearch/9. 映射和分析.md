先看一个现象：
    GET /_search?q=2014   可能是12个结果
  GET /_search?q=date:2014-09-15 只有一个结果
这个是因为_all的时候是链接成了字符串，指定字段的时候被认为是date类型。
curl -XGET 'localhost:9200/thecover/_mapping/blog?pretty'
查看索引下面类型的模式。
 "properties" : {
          "date" : {
            "type" : "date",
            "format" : "yyyy/MM/dd HH:mm:ss||yyyy/MM/dd||epoch_millis"
          },
          "text" : {
            "type" : "string"
          },
          "timestamp" : {
            "type" : "long"
          },
如上。
#确切值VS全文本
确切值就是sql了where title="".  而全文的我们不会为这片文章是否匹配要求，而是会问，这片文章和查询的匹配程度如何。
除了相关度外，我们希望es能够理解我们。
比如：
    UK， 可以返回United Kindom
jump 能返回jumped jumping 甚至leap
fox news huntings 能返回跟fox 和猎人相关的新闻故事。
为了方便进行这类型的查询es会先进行文本分析，然后使用结果建立一个倒排索引。

# 倒排索引
    使用这个结构来做快速的全文搜索，  由唯一的单词列表和每个单词在文档中的顺序组成。
比如：两个文档的content字段：
The quick brown fox jumped over the lazy dog
Quick brown foxes leap over lazy dogs in summer
先分词，然后倒排之后：
Term	Doc_1	Doc_2
Quick		X
The	X	
brown	X	X
但是这样对于近义词和和单复数的时候匹配效果不好。
所以我们统一成一个标准格式。
索引变成：
Term	Doc_1	Doc_2
brown	X	X
dog	X	X
fox	X	X
我们用"+Quick +fox"搜索的时候还是失败，因为没有Quick的匹配，所以我们需要在全文搜索的时候把查询字符串也做一下上面的统一标准格式的处理。
索引文本和查询字符串都要标准化为相同的形式
这个标记化和标准化的过程叫做分词。analysis

# 分析和分析器
   这样的过程：
        1. 标记一个适合作为倒排索引的单独的词
        2. 标准化这些词为标准形式，提高他们的可搜索性和查全率
分析器干了这件事，主要分三个功能：
    字符过滤器    标记前处理字符串，比如去除html标记，&转化为and等
分词器    根据空格或者,号将词分开。中文就不行
标记过滤  大小写，同义词转换等。

多种内建分析器：
    标准分析器：  根据空格分隔，转化大小写
    空格分析器：不转化大小写
    语言分析器：一定的语法规则，比如副词会被去掉
    
查询的时候，
        如果是一个全文字段，就会按照分词器分词然后查找。如果是一个确切值就不会分词了。 比如之前的那个date字段就不会分词。

测试分词器：
    curl -XGET 'localhost:9200/_analyze?analyzer=standard&pretty' -d'{"text":"Text to analyze"}'
{
  "tokens" : [ {
    "token" : "text",
    "start_offset" : 0,
    "end_offset" : 4,
    "type" : "<ALPHANUM>",
    "position" : 0
  }, {
    "token" : "to",
    "start_offset" : 5,
    "end_offset" : 7,
    "type" : "<ALPHANUM>",
    "position" : 1
  }, {
    "token" : "analyze",
    "start_offset" : 8,
    "end_offset" : 15,
    "type" : "<ALPHANUM>",
    "position" : 2
  } ]
}

# 映射
查看映射：curl -XGET 'localhost:9200/thecover/_mapping/blog?pretty'

自定义映射
    分不分词， 比如中国，是中，国分开还是中国一个词
特殊的语言分词器，比如中文的断词段句方式不一样
优化匹配格式
自定义日期格式
上面那个查询映射的一般只有一个type属性。 其实还有一些其他的属性：
index: 有 analyzed,这个意思是用全文索引的方式分析这个字段，not_analyzed意思是索引这个字段使他可以被搜索，但是是一个确认值。 no标示不能被搜索

analyzed:可以指定分词器
curl -XPUT  'localhost:9200/thecover/_mapping/blog?pretty' -d'{
    "properties" : {
          "tag" : {
            "type" : "string",
            "index" : "not_analyzed"
          }
    }
}'
注意不能修改原有字段的，只能新增字段，这个时候需要重新插入一个带有tag属性的值才能看到效果
curl -XPOST 'localhost:9200/thecover/blog/?pretty' -d '{
  "title": "My third blog entry",
  "tag":  "kind 1"
}'
curl -XGET 'localhost:9200/thecover/_search?q=tag:kind+1&pretty'才能查得到
curl -XGET 'localhost:9200/thecover/_search?q=tag:kind&pretty' 就查不到了

curl -XGET 'localhost:9200/thecover/_analyze?field=title&text=kind+1&pretty' 产生两个分词
curl -XGET 'localhost:9200/thecover/_analyze?field=tag&text=kind+1&pretty' 产生一个分词

# 复合类型
数组，不影响查询， 会产生多个词， 数组的值的类型应该一致

多层对象
比如存入一个大的JSON， 会被识别出多个properties
比如存入：
curl -XPOST 'localhost:9200/thecover/user/?pretty' -d '{
    "tweet":            "Elasticsearch is very flexible",
    "user": {
        "id":           "@johnsmith",
        "gender":       "male",
        "age":          26,
        "name": {
            "full":     "John Smith",
            "first":    "John",
            "last":     "Smith"
        }
    }
}'

然后查看结构
会被.号分割为扁平化结构：
curl -XGET 'localhost:9200/thecover/user/_search?q=user.name.full:john&pretty'



